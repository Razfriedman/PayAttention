{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PayAttention_Interactive_NoteBook.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rbn9o02vY0l0","colab_type":"text"},"source":["![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSlqqeSaPXhlIIa3cgWd0l3TBUlzXk5rGIQZmMVheyiLF2VK001&usqp=CAU)\n","\n","# *Emotion Detection Video Analysis Interactive Notebook*\n","\n","Author: *Raz Friedman (r.friedman@berkeley.edu)*\n","\n","Contributors: *Matthew Forbes (mforbes97@berkeley.edu), Wesley Kwong (weskwong2@ischool.berkeley.edu)*   "]},{"cell_type":"markdown","metadata":{"id":"7A6MiYQcwzAj","colab_type":"text"},"source":["## Packages to be installed:"]},{"cell_type":"code","metadata":{"id":"JFA82pmes_Jw","colab_type":"code","colab":{}},"source":["!pip install face_recognition"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o9RFbhwqtnZY","colab_type":"code","colab":{}},"source":["!pip install Pillow"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"crExHKjMtrv-","colab_type":"code","colab":{}},"source":["!pip install EmoPy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cfxhdl-QtuY8","colab_type":"code","colab":{}},"source":["pip install scipy==1.1.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4kph82TOuNnJ","colab_type":"code","colab":{}},"source":["!pip install opencv-python"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pzI8UEPQmhLQ","colab_type":"text"},"source":["##Functions:"]},{"cell_type":"code","metadata":{"id":"mhJ3USqVtxkq","colab_type":"code","colab":{}},"source":["#run this cell\n","\n","%tensorflow_version 1.x\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","import warnings\n","import shutil\n","import os\n","import matplotlib.pyplot as plt\n","import re\n","import numpy as np\n","import pandas as pd\n","import scipy.misc\n","from EmoPy.src.fermodel import FERModel\n","from pkg_resources import resource_filename\n","import face_recognition\n","from PIL import Image\n","import cv2\n","import io\n","import datetime\n","from contextlib import redirect_stdout\n","from google.colab import files\n","\n","\n","def movieToPic(video_path, interval, output_path):\n","    '''Creates images from a video based on interval and stores them in image folder\n","    @movie_path: path to video that will be analyzed\n","    @interval: number of seconds between images to be taken from video\n","    @output_path: where the images will be stored'''\n","\n","    video = cv2.VideoCapture(video_path)\n","    os.chdir(output_path) \n","    fps = video.get(cv2.CAP_PROP_FPS)\n","    frame_count = video.get(cv2.CAP_PROP_FRAME_COUNT)\n","    duration = float(frame_count) / float(fps)\n","    duration = int(round(duration))\n","    fps = int(round(fps))\n","    frames_sec = [i * fps for i in np.arange(1,duration + 1)]\n","    first = frames_sec[0]\n","    last = frames_sec[-1]\n","    frames_num = np.arange(1,len(frames_sec)+1)\n","    frames_sec = frames_sec[1:-1]\n","    frames_num = frames_num[1:-1]\n","    frames_NEED = [x for x,y in zip(frames_sec, frames_num) if y%interval == 0]\n","    final = [first] + frames_NEED + [last]\n","\n","    def time(count, fps):\n","      curr = count / fps\n","      return datetime.timedelta(seconds = curr)  \n","    \n","    count = 0\n","    success,image = video.read()\n","    success = True\n","    while success:\n","      success,image = video.read()\n","      if count in final:\n","        cv2.imwrite(\"%s.jpg\" % time(count, fps), image)\n","      count += 1\n","\n","def face_extract(image_path, out_path):\n","  '''Extracts faces from main image and stores them in sub-image folder.\n","     @image_path - path of image that will be analyzed\n","     @out_path - path of output saved to disk'''\n","\n","  try:\n","    output = []\n","    im_name = image_path.replace(\"/\",\"\").replace(\n","        \"inputs_PayAttention\",\"\").replace(\n","            \".jpg\", \"\")\n","    path = im_name + \"_faces\"\n","    \n","    try:\n","        os.mkdir(out_path + \"/\" + path)\n","    except FileExistsError:\n","        pass\n","    image = face_recognition.load_image_file(image_path)\n","    face_locations = face_recognition.face_locations(image)\n","    for tup in face_locations:\n","        output.append((tup[3],tup[0],tup[1],tup[2]))\n","\n","    img = Image.open(image_path)\n","    \n","    num = 1\n","    for i in output:\n","        name = \"sub_\" + str(num) + \".jpg\"\n","        sub = img.crop((i[0],i[1],i[2],i[3])).resize((\n","            128,128)).save(out_path + '/' + path + \"/\" + name, optimize=True)\n","        num +=1\n","    return im_name + \" \" + \"- Extraction Complete!\"\n","  except:\n","    print(\"Extraction Failed\")\n","\n","def emote_img(path, target_emos):\n","  ''' Quantifies target emotions for a given image.\n","      @path - path of image that will be analyzed\n","      @target_emos - target emotions selected by user from emotion subsets ''' \n","  \n","  sub_names = []\n","  model = FERModel(target_emos, verbose=False)\n","  for sub in os.listdir(path):\n","    print(sub)\n","    model.predict(path + \"/\" + sub)\n","    sub_names += [sub]\n","  return sub_names\n","\n","def img_to_df(path, target_emos, out):\n","  ''' Converts a given image into a dataframe of analyized emotions.\n","      @path - path of image that will be analyzed\n","      @target_emos - target emotions selected by user from emotion subsets\n","      @out - string represention of model predictions '''\n","\n","  output = \"/output_PayAttention\"\n","  df_path = output + \"/dataframes\"\n","  \n","  try:\n","    os.mkdir(df_path)\n","  except FileExistsError:\n","    pass\n","  \n","  values = []\n","  for emo in target_emos:\n","    regex = r\"\" + emo + \": (\\d+.\\d)\"\n","    z = re.findall(regex, out)\n","    values += [z]\n","\n","  subs_num = 0\n","  for i in os.listdir(path):\n","    subs_num += 1\n","\n","  ordered = []\n","  for i in np.arange(subs_num):\n","    for j in np.arange(len(values)):\n","      ordered += [float(values[j][i])]\n","  try:\n","    df = pd.DataFrame(np.split(np.array(ordered), subs_num),\n","                    columns = target_emos)\n","  except ZeroDivisionError:\n","    df = pd.DataFrame(data = np.zeros((1,len(target_emos))), columns = target_emos)\n","  df = df.div(100)\n","  df.to_csv(df_path + \"/\" + path.replace(output + \"/\", \"\") \n","  + \"_df.csv\", index = False, header=True)\n","  pd.option_context('display.max_rows', None, 'display.max_columns', None)\n","  return display(df)\n","\n","def create_agg_df(path_dataframes, target_emotions):\n","  ''' Creates an aggregate dataframe from all dataframes.\n","      @path_dataframes - path to df folder\n","      @target_emotions - target emotions selected by user from emotion subsets\n","     '''\n","  output = \"/output_PayAttention\"\n","  vals = []\n","  names = []\n","  for i in sorted(os.listdir(path_dataframes)):\n","    df = pd.read_csv(path_dataframes + \"/\" + i)\n","    name = i.replace(\"_faces\", \"\").replace(\".csv\", \"\")\n","  \n","    dom_emotion = \"\"\n","    curr = 0\n","    for col in target_emotions:\n","      if sum(df[col])/df.shape[0] > curr:\n","        dom_emotion = col\n","      if sum(df[col])/df.shape[0] == curr:\n","        dom_emotion += \" & \" + col\n","\n","    emo_vals = np.array(df.describe().iloc[1, :]).round(3)\n","    full = np.append(emo_vals, dom_emotion)\n","    names += [name]\n","    vals += [full]\n","\n","  agg = pd.DataFrame(vals,\n","                     columns = np.append(target_emotions, \"Dominant Emotion\"),\n","                     index = names)\n","  agg.to_csv(path_dataframes + \"/\" + \"aggregate.csv\", index = True, header=True)\n","  pd.option_context('display.max_rows', None, 'display.max_columns', None)\n","  display(agg)\n","\n","def EDA(path_dataframes, target_emotions):\n","  ''' Performs exploratory data analysis on given dataframes.\n","      @path_dataframes - path to df folder\n","      @target_emotions - target emotions selected by user from emotion subsets\n","      '''\n","  output = \"/output_PayAttention\"\n","  vis = output + \"/data_visualizations\"\n","  try:\n","    os.mkdir(vis)\n","  except FileExistsError:\n","    pass\n","\n","  agg = pd.read_csv(path_dataframes + \"/aggregate.csv\")\n","  agg = agg.rename(columns={'Unnamed: 0': 'Image'})\n","  agg = agg.set_index(\"Image\")\n","  no_dom = agg.drop(columns=['Dominant Emotion'])\n","  dom = agg[\"Dominant Emotion\"].to_frame(0).groupby(0)[0].count()\n","  for emo in target_emotions:\n","    if emo not in dom.index:\n","      s = pd.Series([0])\n","      dom = dom.append(s)\n","      dom = dom.rename({0: emo})\n","  dom = dom.to_frame()\n","  \n","  print()\n","  for emo in target_emotions:\n","    print(\" ** For emotion \" + emo \n","        + \" the max value of \" + str(agg[emo].max()) \n","        + \" is in image \" + agg[emo].idxmax().replace(\"_df\", \"\"))\n","    print()\n","  \n","  no_dom.boxplot(figsize=(10,8), fontsize=15)\n","  plt.title(\"Boxplot - \" + str(target_emotions), fontsize=16)\n","  plt.savefig(vis+ \"/Boxplot\")\n","\n","  no_dom.plot.barh(figsize=(10,8), fontsize=15)\n","  plt.title(str(target_emotions) + \" Per Image\", fontsize=16)\n","  plt.ylabel(\"Images\", fontsize=16)\n","  plt.savefig(vis + \"/Barh_Per_Emotion\")\n","\n","  no_dom.plot.line(figsize=(10,8), grid = True)\n","  plt.title(\"Changes In \" + str(target_emotions) + \" Over Time\", fontsize=16)\n","  plt.xlabel(\"Images Over Time\", fontsize=16)\n","  plt.savefig(vis + \"/Linegraph_emotions_over_time\")\n","\n","  dom.plot.bar(figsize=(10,8), fontsize=16, legend = False, rot = 0)\n","  plt.xlabel(\"Emotions\", fontsize=16)\n","  plt.title(\"Dominant Emotion Count\", fontsize=16)\n","  plt.savefig(vis + \"/Bar_Dom_Emotion\")\n","\n","def helper_output_emo(emo_subset):\n","  ''' Helper function for pipline.\n","      @emo_subset - target emotions selected by user from emotion subsets\n","      '''\n","  f = io.StringIO()\n","  with redirect_stdout(f):\n","    for i in sorted(os.listdir('/output_PayAttention')):\n","      emote_img(\"/output_PayAttention\" + \"/\" + i, emo_subset)\n","  out = f.getvalue()\n","  return out\n","\n","def pipeline(vid_path, vid_interval, emo_subset):\n","  ''' Function that runs through entire emotion analysis pipeline.\n","      @vid_path - path to video\n","      @vid_interval - analysis interval(seconds) duration\n","      @emo_subset - target emotions selected by user from emotion subsets\n","     '''\n","  inputs = \"/inputs_PayAttention\"\n","  output = \"/output_PayAttention\"\n","  os.mkdir(inputs)\n","  os.mkdir(output)\n","  try:\n","    movieToPic(vid_path, vid_interval, inputs)\n","  except:\n","    pass\n","  if len(os.listdir(inputs + '/') ) == 0:\n","    return \"Directory is empty - Please upload video\"\n","  for i in sorted(os.listdir(inputs)):\n","    print(face_extract(inputs + '/' + i, output))\n","  print()\n","  print(\"**********************************************\")\n","  print()\n","  print()\n","  out = helper_output_emo(emo_subset)\n","  path_list = []\n","  for i in sorted(os.listdir(output)):\n","    path_list += [i]\n","\n","  emo_vals = out.split('Initializing')\n","  emo_vals.pop(0)\n","  emo_vals\n","  for tup in zip(path_list, emo_vals):\n","    print(tup[0])\n","    print(\"-------------------------------\")\n","    img_to_df(output + \"/\" + tup[0], emo_subset, tup[1])\n","    print()\n","  print(\"Aggregate DataFrame\")\n","  print(\"================================\")\n","  print()\n","  create_agg_df(output + \"/dataframes\", emo_subset)\n","  EDA(output + \"/dataframes\", emo_subset)\n","\n","  # DELETES ALL INPUTS & OUTPUTS\n","  shutil.rmtree('/output_PayAttention')\n","  shutil.rmtree('/inputs_PayAttention')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ItqI2SczuOUu","colab_type":"text"},"source":["## Notebook UI"]},{"cell_type":"markdown","metadata":{"id":"E1t6D_GB3Fan","colab_type":"text"},"source":["*User*, please upload video.mp4 for analysis"]},{"cell_type":"code","metadata":{"id":"_gcUn0n1WJqR","colab_type":"code","colab":{}},"source":["vids = files.upload()\n","display(vids)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FycnETqTic4d","colab_type":"text"},"source":["*User*, please select interval (in seconds) by which video will be analyized\n","\n","*Example: 2 = video analysis performed every 2 seconds*"]},{"cell_type":"code","metadata":{"id":"SjuiN8BAYNHx","colab_type":"code","colab":{}},"source":["import ipywidgets as widgets\n","from IPython.display import display\n","\n","inter = widgets.IntText(\n","    value=1,\n","    description='Interval:',\n","    disabled=False\n",")\n","\n","display(inter)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FttQ4jN-nCFu","colab_type":"text"},"source":["*User*, please select emotion subset for analysis"]},{"cell_type":"code","metadata":{"id":"0QLXNQ0xk4w-","colab_type":"code","colab":{}},"source":["emo = widgets.RadioButtons(\n","    options=['anger,fear,surprise,calm', 'disgust,surprise,happiness',\n","             'anger,fear,surprise', \"anger,fear,calm\", \"anger,happiness\",\n","             \"anger,happiness,calm\", \"disgust,anger,fear\",\n","             \"disgust,surprise,calm\", \"disgust,sadness,surprise\"],\n","    layout={'width': 'max-content'},\n","    description='Emotions:',\n","    disabled=False\n",")\n","\n","display(emo)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9O0O94r8JCus","colab_type":"text"},"source":["### *Run this cell for results:*"]},{"cell_type":"code","metadata":{"id":"FfEYKuRD5ukH","colab_type":"code","colab":{}},"source":["for vid in vids.keys():\n","  pipeline(os.path.abspath(vid), inter.value, emo.value.split(\",\"))"],"execution_count":0,"outputs":[]}]}